\section{Численные методы решения обратных задач на основе решения задачи оценивания} \label{chapt3_Algorithms}
Полученные выше теоретические результаты позволяют решить обратную задачу для частных скрытых полумарковских моделей, таких как скрытая полумарковская модель Фергюсона и скрытая полумарковская $QP$-модель. Напомним, что под обратной задачей мы понимаем задачу подбора по зарегистрированной в канале последовательности ошибок математической модели, которая с наибольшей вероятностью могла бы породить такую последовательность. В этом разделе опишем особенности алгоритма решения задачи оценивания, сформулируем алгоритм решения обратной задачи, приведем иллюстрирующие примеры.

%... Полученные выше (..? теоретические...) результаты позволяют решить обратную задачу для ..., т.е. есть поток ошибок, набор моделей, требуется подобрать наиболее подходящую модель для потока...В случае модели определенного вида (...предыдущие 3 раздела...) имеются "формульные результаты". Опишем особенности алгоритма реализации задачи оценивания (сегменты)... опишем алгоритм решения обратной задачи с учетом базы. Примеры (алгоритм работает), примеры (с выделением отрезков лучше), примеры (может выделять)... Может быть в этом разделе нет "реальных" \textbf{помех}.


\subsection{Алгоритм решения задачи оценивания} \label{chapt3_Evaluation_Problem}
Приведем формальное описание алгоритма решения задачи оценивания для скрытой полумарковской модели на примере скрытой полумарковской $QP$-модели.

\begin{algorithm}[H]
\SetAlgoLined
\caption{${\mathrm{EvaluationProblemSolver}}$}
\KwData{$O=O_1O_2 ... O_T$ -- последовательность ошибок, $\lambda$ -- скрытая полумарковская модель источников ошибок.}
\KwResult{вероятность генерации последовательности $O$ моделью $\lambda$ -- $P(O|\lambda)$.}

Вычислить $P(O|\lambda) = \widetilde{P}[O_{1:T}]$ по следующим формулам:
\begin{equation}\nonumber
\widetilde{P}[O_{1:T}]=\sum\limits_{j\in \mathcal{S}}\sum\limits_{d \in \mathcal{D}}\sum\limits_{d_1=1}^{d}P[O_{1:T-d_1}]\overline{\alpha}_{T-d_1+d}(j,d)b_{j,d}(O_{T-d_1+1}^{T-d_1+d}),
\end{equation}
где
\begin{equation}\nonumber
\widetilde{P}[O_{1:t}]=\left\{\begin{array}{ll}
1, &t\leq 0,\\\sum\limits_{j\in \mathcal{S}}\sum\limits_{d \in \mathcal{D}}\sum\limits_{d_1=1}^{d}\widetilde{P}[O_{1:t-d_1}]\overline{\alpha}_{t-d_1+d}(j,d)\prod\limits_{\tau=t-d_1+1}^{t-d_1+d} b_{i,d}^\tau(O_\tau),& t\in [1,T],\end{array}\right.
\end{equation}
\begin{equation}\nonumber
\overline{\alpha}_{t}(i,d)=\left\{
\begin{array}{ll}
\pi_i p_i(d), &t\leq 0,\\
\sum\limits_{i^\prime\in \mathcal{S}}\sum\limits_{d^\prime \in \mathcal{D}}\overline{\alpha}_{t-d}(i^\prime,d^\prime)
\prod\limits_{\tau=t-d-d^\prime+1}^{t-d}b_{i^\prime,d^\prime}^\tau(O_\tau)\frac{P[O_{1:t-d-d^\prime}]}{P[O_{1:t-d}]}
a_{i^\prime i} p_i(d), & t>0,
\end{array}\right.
\end{equation}
$$b_{i,d}^{\tau}(O_\tau) =I_{\mathbb{F}^{\ast}_q}(O_\tau)\varphi_i^d(\tau)\mu_idb_i(O_\tau)+(1-I_{\mathbb{F}^{\ast}_q}(O_\tau))(1-\varphi_i^d(\tau)\mu_id).$$

\KwRet $P(O|\lambda) = \widetilde{P}[O_{1:T}]$

\end{algorithm}

Заметим, что аналогично можно решать задачу оценивая для скрытой полумарковской модели Фергюсона. Алгоритм, аналогичный EvaluationProblemSolver для скрытой полумарковской $QP$-модели, для модели Фергюсона может быть получен использованием формул теоремы \ref{chapt3_theorem_Ferguson}.

Ранее в разделе \ref{chapt1_GHSMM_IP_Yu} отмечалось, что в работе \cite{Yu} сделан вывод о том, что решение
задачи оценивания с использованием апостериорных вероятностей позволяет избежать проблемы антипереполнения. Однако, использование апостериорных вероятностей на самом деле проблему антипереполнения решает не полностью. 

В работе предлагается усредняющий алгоритм, в основе которого лежит деление исходной последовательности на части и усреднение вероятностей наблюдения этих частей.

\begin{algorithm}[h]
\SetAlgoLined
\caption{${\mathrm{EvaluationProblemSolverExtended}}$}
\KwData{

1) последовательность ошибок $O=O_1O_2 ... O_T$;

2) скрытая полумарковская модель $\lambda$;

3) длина сегмента разбиения $l\leq T$.
}
\KwResult{усредненная вероятность генерации последовательности $O$ моделью $\lambda$.}

Разбить $O$ на $n=\lfloor T/l \rfloor$ частичных последовательностей $\{O_i\}_{i=1}^n$, где под $\lfloor T/l \rfloor$ будем понимать ближайшее целое число, не превосходящее $T/l$.

\For{всех i из (0, n]}{вычислить вероятность $P(O_i|\lambda)=\widetilde{P}[O_i|\lambda]$ в соответствии с алгоритмом InverseProblemSolver}

Вычислить усредненную вероятность $P_{avg}(O|\lambda)=\frac{\sum\limits_{i\in (0,n]}P(O_i|\lambda)}{n},$
\KwRet $P_{avg}(O|\lambda)$.

\end{algorithm}

 Заметим, что правильный выбор длины сегмента является важным условием корректной работы метода.
 При выборе длины сегмента нужно учитывать, что при слишком маленькой длине нельзя гарантировать точность метода, а при слишком большой длине вероятности могут оказаться за пределом вычислительной точности.

 TODO: доработать рекомендации к выбору длины сегмента.

\subsection{Алгоритм решения обратной задачи} \label{chapt3_Inverse_Problem}
Предположим, что есть набор скрытых полумарковских моделей с различными параметрами $\Lambda$, о котором далее будем говорить как о базе моделей, и имеется запись последовательности наблюдений $O=O_1,...O_T$.

Под задачей идентификации скрытой полумарковской модели (обратной задачей) будем понимать задачу подбора для последовательности $O$ такой модели из базы моделей $\Lambda$, которая позволит генерировать последовательности, близкие к заданной в смысле выбранного критерия.

В качестве критерия выбора модели может быть использован один из следующих возможных критериев.

1) Критерием максимального правдоподобия называется критерий
$$\lambda_{max} = \arg\max\limits_{\lambda\in\Lambda} P(O | \lambda)$$

2) Под критерием максимума относительной веротности между последовательностью $O$ и полным набором моделей $\Lambda$ будем называть критерий
$$\lambda_{max} =\arg\max\limits_{\lambda\in\Lambda} (\frac{P(\lambda, O_\mu)}{\sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu)}). $$

3) Критерием максимума средней взаимной информации между последовательностью $O$ и полным набором моделей $\Lambda$ называется критерий
$$\lambda_{max} =\arg\max\limits_{\lambda\in\Lambda} [\log(P(\lambda, O_\mu) - \log \sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu))]= \arg\max\limits_{\lambda\in\Lambda} log(\frac{P(\lambda, O_\mu)}{\sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu)}). $$

Рассмотрим метод решения задачи подбора, основаный на вычислении для каждой модели $\lambda$ из базы $\Lambda$ вероятности $P(O | \lambda)$ наблюдения некоторой реально полученной последовательности ошибок $O=O_1O_2 ... O_T$ и сравнении этих вероятностей для разных моделей. В качестве искомой модели выберем модель, дающую наибольшую вероятность.

Сформулируем следующий алгоритм подбора, в котором в качестве критерия выбора модели используется критерий максимального правдоподобия.

\begin{algorithm}[h]
\SetAlgoLined
\caption{${\mathrm{InverseProblemSolver}}$}
\KwData{

1) последовательность ошибок $O=O_1O_2 ... O_T$;

2) база $\Lambda$, составленная из скрытых полумарковских моделей источников ошибок с различными параметрами.
}
\KwResult{одель $\lambda_{max}$, позволяющая наилучшим образом имитировать ошибку в канале.}

\For{всех $\lambda \in \Lambda$}{вычислить вероятность $P_{avg}(O|\lambda)$ в соответствии с алгоритмом EvaluationProblemSolverExtended}

Найти $P_{max}=\max\limits_{\lambda_m\in \Lambda}{P(O | \lambda_m)}.$

Выбираем модель, такую что
 $$\lambda_{max}=\arg\max\limits_{\lambda_m\in \Lambda} P(O | \lambda_m) = \arg P_{max},$$
где функция $\arg$ возвращает модель $\lambda_{max}$, соответствующую максимальной вероятности $P_{max}$.
\KwRet $\lambda_{max}$.

\end{algorithm}

Описанный алгоритм InverseProblemSolver схематически описан на рис. \ref{chapt_3_inverse_problem_algoritm}.

\begin{center}
\label{chapt_3_inverse_problem_algoritm}
  % Requires \usepackage{graphicx}
  \includegraphics[width=160mm, height = 100mm]{/part3/ris_dip.pdf}\\
  Рис. 6. Блок-схема алгоритма подбора параметров.
\end{center}

Проиллюстрируем предложенный алгоритм на небольшом примере

\subsubsection{Пример}
Рассмотрим три СПМQP-модели с одинаковым выходным алфавитом ${\mathbb{F}}_2$:
1. $\bar{\lambda} =  \{\mathcal{\bar{S}};\mathcal{\bar{D}}; \bar{A}; \bar{\Pi}; \bar{p}(d); \bar{\rho}; \bar{\mu};\bar{M} ;{\mathbb{F}}_2;\bar{B} \},$ где
$$\bar{\Pi}=\{0.5,0.5\}; \bar{M}=0.1; \bar{\mu}=\{0.1,0.16\}.$$
\[ \bar{A}=\left(
  \begin{array}{cc}
    0.999 & 0.001 \\
    0.001 & 0.999 \\
  \end{array}
\right); \bar{B}=\left(
  \begin{array}{cc}
    0.8 & 0.2 \\
    0.3 & 0.7 \\
  \end{array}
\right); \]
\begin{equation}\nonumber
\begin{split}
&\bar{\rho}_0=\{0.004, 0.004, 0.007, 0.05, 0.24, 0.39, 0.24, 0.05, 0.007, 0.004, 0.004\},\\
&\bar{\rho}_1=\{0.67, 0.26, 0.05, 0.007, 0.006, 0.004, 0.001, 0.001, 0.001\},
\end{split}
\end{equation}
\[\bar{p}_0(d) =\left(
  \begin{tabular}{l|c}
    $10$ &$ 15$  \\
    \hline
    $0.1$ & $0.9$ \\
  \end{tabular}
    \right);
\bar{p}_1(d) =\left(
  \begin{tabular}{l|c}
    $9$ &$ 8$  \\
    \hline
    $0.5$ & $0.5$ \\
  \end{tabular}
    \right);
\]

2. $\hat{\lambda} =  \{\mathcal{\hat{S}};\mathcal{\hat{D}}; \hat{A}; \hat{\Pi}; \hat{p}(d); \hat{\rho}; \hat{\mu};\hat{M} ;{\mathbb{F}}_2;\hat{B} \},$ где
$$\hat{\Pi}=\{0.9,0.1\}; \hat{M}=0.1; \hat{\mu}=\{0.1,0.16\}.$$
\[ \hat{A}=\left(
  \begin{array}{cc}
    0.5 & 0.5 \\
    0.5 & 0.5 \\
  \end{array}
\right); \hat{B}=\left(
  \begin{array}{cc}
    0.993 & 0.007 \\
    0.007 & 0.993 \\
  \end{array}
\right); \]
\begin{equation}\nonumber
\begin{split}
&\hat{\rho}_0=\{0.001, 0.001, 0.001, 0.001, 0.001, 0.007, 0.007, 0.007, 0.007,\\& 0.007, 0.04, 0.05, 0.24, 0.39, 0.24\},\\
&\hat{\rho}_1=\{0.001, 0.001, 0.001, 0.004, 0.006, 0.007, 0.05, 0.26, 0.67\},
\end{split}
\end{equation}
\[\hat{p}_0(d) =\left(
  \begin{tabular}{l|c}
    $10$ &$ 20$  \\
    \hline
    $0.9$ & $0.1$ \\
  \end{tabular}
    \right);
\hat{p}_1(d) =\left(
  \begin{tabular}{l}
    $9$  \\
    \hline
    $1$  \\
  \end{tabular}
    \right);
\]
3. $\tilde{\lambda} =  \{\mathcal{\tilde{S}};\mathcal{\tilde{D}}; \tilde{A}; \tilde{\Pi}; \tilde{p}(d); \tilde{\rho}; \tilde{\mu};\tilde{M} ;{\mathbb{F}}_2;\tilde{B} \},$
где
$$\tilde{\Pi}=\{0.2,0.8\}; \tilde{M}=0.1; \tilde{\mu}=\{0.2,0.2\}.$$
\[ \tilde{A}=\left(
  \begin{array}{cc}
    0.75 & 0.25 \\
    0.25 & 0.75 \\
  \end{array}
\right); \tilde{B}=\left(
  \begin{array}{cc}
    0.5 & 0.5 \\
    0.5 & 0.5 \\
  \end{array}
\right); \]
\begin{equation}\nonumber
\begin{split}
&\tilde{\rho}_0=\{0.495, 0.002, 0.001, 0.001, 0.001, 0.495, 0.001, 0.001, 0.001, 0.002\},\\
&\tilde{\rho}_1=\{0.002, 0.001, 0.001, 0.495, 0.495, 0.001, 0.001, 0.001, 0.001, 0.002\},
\end{split}
\end{equation}
\[\tilde{p}_0(d) =\left(
  \begin{tabular}{l|c}
    $13$ &$ 18$  \\
    \hline
    $0.3$ & $0.7$ \\
  \end{tabular}
    \right);
\tilde{p}_1(d) =\left(
  \begin{tabular}{l|c}
    $22$ &$ 15$  \\
    \hline
    $0.8$ & $0.2$ \\
  \end{tabular}
    \right);
\]
Проведем следующий эксперимент. Используя каждую из моделей $\bar{\lambda}$, $\hat{\lambda}$, $\tilde{\lambda}$ в качестве генератора, сгенерируем три последовательности $O_{\bar{\lambda}}$, $O_{\hat{\lambda}}$,$O_{\tilde{\lambda}}$ длиной 5000 символов. Рассмотрим последовательность $\bar{\lambda}$. Для удобства вычислений разобьем ее на 10 сегментов по 500 символов и, используя формулы (), вычислим вероятность генерации моделью $\bar{\lambda}$ каждого из 10 сегментов, после чего усредним полученные значения и тем самым вычислим $P(\bar{\lambda}, O_{\bar{\lambda}})$. Аналогичные вычисления проведем для всех пар $(\lambda, O_\mu)$, где $\lambda, \mu \in \{\bar{\lambda}, \hat{\lambda}, \tilde{\lambda}\}$. Полученные девять чисел $P(\lambda, O_\mu)$ приведем в таблице 1.

\begin{mtable}
Значения величины $P(\lambda, O_\mu)$ для рассматриваемого примера
\begin{center}
\begin{tabular}{|l|l|l|l|}
  \hline
  - & $O_{\bar{\lambda}}$ & $O_{\hat{\lambda}}$ & $O_{\tilde{\lambda}}$ \\
  \hline
  $\bar{\lambda}$ & $3.42E-57$
 & $1.67E-94$
 & $1.70E-137$ \\
  $\hat{\lambda}$ & $2.03E-90$ & $9.45E-49$ & $1.45E-211$ \\
  $\tilde{\lambda}$ & $7.85E-146$ & $5.08E-148$ & $2.26E-55$ \\
  \hline
\end{tabular}
\end{center}
\end{mtable}

Поставим задачу по таблице результатов выбрать модель, наиболее соответствующую каждой из последовательностей. В качестве критерия можно выбрать критерий максимального правдоподобия, то есть выбирать модель дающую наибольшую вероятность $P(\lambda, O_\mu)$. Если пользоваться этим критерием, видно, что для каждой из последовательностей модель, которой она была сгенерирована, дает наибольшую вероятность. Однако, зачастую удобно использовать и другие критерии. Например, выбирать модель, дающую максимальную относительную вероятность, то есть $\frac{P(\lambda, O_\mu)}{\sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu)}$. В некоторых случаях удобнее сравнивать не сами относительные вероятности, а их логарифмы. Таким образом, выбирается модель, дающая на последовательности максимум $log(\frac{P(\lambda, O_\mu)}{\sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu)})$. Этот критерий носит название критерия максимума взаимной информации. Для рассматриваемого примера значения этой величины приведены в таблице:

\begin{mtable}
Значения величины $log(\frac{P(\lambda, O_\mu)}{\sum\limits_{\lambda\in \Lambda}P(\lambda, O_\mu)})$ для рассматриваемого примера
\begin{center}
\begin{tabular}{|l|l|l|l|}
  \hline
  - & $O_{\bar{\lambda}}$ & $O_{\hat{\lambda}}$ & $O_{\tilde{\lambda}}$ \\
  \hline
  $\bar{\lambda}$ & $0$ & $-45.75372824$ & $-82.12334112$ \\
  $\hat{\lambda}$ & $-33.22597501$ & $0$ & $-156.1912406$ \\
  $\tilde{\lambda}$ & $-88.63917916$ & $-99.26931236$ & $0$ \\
  \hline
\end{tabular}
\end{center}
\end{mtable}

Из таблицы видно, что критерий максимума взаимной информации в нашем примере позволяет сделать такой же вывод, что и критерий максимального правдоподобия.

\section{Комплекс программ для решения задачи подбора скрытой полумарковской модели источника ошибок}\label{chapt3_Implementation}
\subsection{Структура программного комплекса}
м б вспомогательные
\subsection{Программная реализация алгоритма решения задачи оценивания для скрытых полумарковских моделей}
\subsection{Программная реализация алгоритма решения задачи подбора скрытой полумарковской модели источника ошибок}
