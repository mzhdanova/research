\section{Обратные задачи в теории скрытых марковских и полумарковских моделей}\label{chapt1_Inverse_Problems}
\subsection{Постановка основных обратных задач теории скрытых марковских и полумарковских моделей.}
 Для использования скрытых марковских и полумарковских моделей при решении прикладных задач выделяют следующие три классические задачи \cite{Rabiner}, \cite{Dymarski_11}. Описываемые задачи одинаково актуальны как для скрытых марковских так и для скрытых полумарковских моделей, поэтому под моделью $\lambda$ будем понимать модель из одного из этих классов.

\textbf{Задача оценивания (The Evaluation Problem).} Пусть задана последовательность наблюдений $O_{1:T}=O_1O_2…O_T$  и  $\lambda$. Как эффективно вычислить величину $P[O_{1:T} | \lambda]$, т.е. вероятность генерации этой последовательности наблюдений данной моделью?

\textbf{Задача декодирования (The Decoding Problem).} Пусть заданы последовательность наблюдений $O_{1:T}=O_1O_2…O_T$  и модель  $\lambda$. Как выбрать последовательность состояний $Q = q_1q_2…q_T$, которая в некотором значимом смысле будет оптимальной (например, наилучшим образом соответствует имеющейся последовательности наблюдений)?

\textbf{Задача обучения (The Lerning Problem)} Каким образом нужно подстроить параметры модели $\lambda$, для того чтобы максимизировать $P[O_{1:T} | \lambda]$?

 Задача оценивания подразумевает, что известны модель и последовательность наблюдений и требуется вычислить вероятность того, что наблюдения порождены указанной моделью. Можно также рассматривать эту задачу, как задачу выяснения, насколько хорошо некоторая модель подходит к имеющейся последовательности наблюдений. Так, например, ее решение оказывается полезным, когда возникает необходимость оптимального подбора из имеющегося набора моделей модели, наиболее точно имитирующей реальное поведение моделируемой системы.

 Задача декодирования связана с тем, каким образом можно найти \"правильную\" последовательность состояний, породившую последовательность наблюдений. Очевидно, что при заданной последовательности наблюдений и известной модели нельза найти абсолютно точную последовательность состояний, соответствующих наблюдениями, за исключением того вырожденного случая, когда имеющаяся последовательность порождена именно этой моделью. Поэтому на практике определение последовательности состояний осуществляется путем использования некоторого критерия оптимальности.

 Задача обучения сводится к оптимизации значений параметров модели для того, чтобы она наиучшим образом соответствовала имеющимся наблюдениям. Подследовательность наблюдений, использующаяся для переоценки параметров носит название обучающей последовательности. Проблема обучения важна для приложений, поскольку в результате ее решения создается модель, наилучшим образом описывающая реальное явление.

Для нас особый интерес в дальнейшем будет представлять решение задачи оценивания. В следующих двух подразделах рассмотрим решение этой задачи для случая скрытой марковской и скрытой полумарковской модели.

\subsection{Решение задачи оценивания для скрытой марковской модели.}
В этом разделе рассмотрим решение задачи оценивания для скрытой марковской модели. Приведем алгоритм решения этой проблемы, впервые предложенный Баумом и описанный в \cite{Baum}, \cite{Rabiner}. Он носит название алгоритма \textit{прямого хода}.

Рассмотрим последовательность наблюдений $O_{1:T}=O_1O_2…O_T$ и скрытую марковскую модель $\lambda= {A, B, \pi}$. Будем полагать, что символы последовательности $O_{1:T}$ принадлежат алфавиту модели $\lambda$. Под $Q = q_1q_2…q_T$ будем понимать последовательность состояний, породивших последовательность $O$.

Введем так называемую \textit{прямую} переменную $\alpha_t(i)$, определяемую выражением
$$\alpha_t(i) = P[O_1O_2…O_t ,q_t=S_i|\lambda].$$

Она представляет собой вероятность появления для данной модели частичной последовательности наблюдений $O_1O_2…O_t$  (до момента $t$ и состояния в этот момент). По индукции $\alpha_t(i)$ определяется следующим образом:

\textbf{Шаг 1.} Инициализация:
$$\alpha_1(i)= \pi_ib_i(O_1),	\;\;\; 1\leq i\leq N.$$

\textbf{Шаг 2.} Индуктивный переход:
\begin{eqnarray} \alpha_{t+1}(j)=[\sum_{i=1}^N \alpha_t(i)a_{ij}] b_j (O_{t+1} ),\nonumber \\
1\leq t \leq T-1, \;\;\; 1\leq j\leq N. \nonumber \\
\end{eqnarray}

\textbf{Шаг 3.} Окончание:
$$P[O_{1:T}|\lambda]=\sum_{i=1}^N \alpha_T(i).$$

Шаг 1 устанавливает значение прямой переменной равным совместной вероятности состояния и начального наблюдения $O_1$ . Индуктивный переход может быть проиллюстрирован следующим образом: $\alpha_t(i)$ представляет собой вероятность события, заключающегося в том, что наблюдается последовательность $O_1O_2\ldots O_t$, причем состояние в момент $t$ есть $S_i$, тогда произведение $\alpha_t(i) a_{ij}$ -- вероятность того, что в следующий момент $t+1$ будет достигнуто состояние $S_j$ . Суммирование этого произведения по всем возможным состояниям $S_i (1\leq i\leq N)$, в момент времени $t$ дает вероятность появления состояния $S_j$ в момент $t+1$ совместно со всеми предыдущими наблюдениями. Определив $S_j$, нетрудно далее убедиться в том, что $\alpha_{t+1}(j)$ получается посредством вычисления вероятности наблюдения $O_{t+1}$ в состоянии $j$, т. е. умножением результата суммирования на вероятность $b_j(O_{t+1})$.

Вычисления по индуктивной формуле при фиксированном $t$ выполняются для всех состояний $j$ ($1\leq j\leq N$) и при $t=1,2, \ldots, T-1$. На шаге 3 вычисляется искомая вероятность $P[O_{1:T}|\lambda]$ как сумма значений прямых переменных $\alpha_{T}(i)$. Это вытекает из того факта, что, по определению,
$$\alpha_{T}(i)=P[O_1 O_2\ldots O_T,q_T=S_i |\lambda].$$

Следовательно, $P[O_{1:T}|\lambda]$ является суммой по $i$ величин $\alpha_{T}(i)$.

\subsection{Решение задачи оценивания для общей скрытой полумарковской модели.} \label{chapt1_GHSMM_IP_Yu}
В этом разделе рассмотрим задачу оценивания для общей скрытой полумарковской модели.

 Зафиксируем некоторую общую скрытую полумарковскую модель $\lambda = \{\mathcal{S}, \mathcal{D}, A, \Pi, \mathcal{V}, B \}$ и некоторую последовательность $O_{1:T}$ над алфавитом $\mathcal{V}$. Далее под задачей оценивания будем понимать задачу вычисления вероятности генерации последовательности $O_{1:T}$ данной моделью $\lambda$. В дальнейшем будем обозначать искомую вероятность $P_{\lambda}[O_{1:T}]=P[O_{1:T}]$. Отметим, что эта вероятность рассматривается при фиксированной модели $\lambda$, однако для удобства далее мы будем, как правило, опускать значок $\lambda$ в обозначении этой величины.

  Не теряя общности будем предполагать, что наблюдаемая последовательность $O_{1:T}$ является частью потенциально бесконечной последовательности символов $O=..,O_{-1},O_0, O_1, .., O_T, ..$. Будем считать, что последовательность $O_{1:T}$ порождена общей скрытой полумарковской моделью, а значит, наблюдаемой последовательности $O$ соответствует скрытая последовательность состояний $R=.., r_{-1}, r_{0}, r_1, .., r_q,..$, где через $r_1$ будем обозначать состояние, породившее первые наблюдаемые символы из $O_{1:T}$, через $r_q$ -- состояние, породившее последние наблюдаемые символы последовательности $O_{1:T}$, при этом предполагается, что $q\leq T$. Далее будем говорить об $r_1$ как о первом наблюдаемом состоянии, а о $r_q$ как о последнем наблюдаемом состоянии.

 Приведем классический алгоритм прямого хода для общей скрытой полумарковской модели, изложенный в \cite{Yu}. Прямая переменная в случае полумарковской модели вводится следующим образом:
 \begin{equation}
 \label{eq:chapter1_GHSMM_InverseProblem_alpha_def}
 \alpha_t(j,d)=P[S_{[t-d+1:t]}=j, o_{1:t}|\lambda].
 \end{equation}

Аналогично формулам алгоритма прямого хода для скрытой марковской модели получаются формула вычисления прямой переменной для общей скрытой полумарковской модели:
 \begin{equation}
 \label{eq:chapter1_GHSMM_InverseProblem_alpha_rec}
 \alpha_t(j,d)=\sum\limits_{i\in S\{j\}}\sum\limits_{d^\prime \in D} \alpha_{t-d}(i,d^\prime)\cdot a_{(i,d^\prime)(j,d)}\cdot b_{j,d}(o_{t-d})
 \end{equation}
 для $t>0$, $d\in D$, $j\in S$.

   Для инициализации рекурсии \cite{Yu} (с. 218-219) предлагает использовать одно из следующих двух предположений.
   Упрощающее предположение (simplifying assumption):

   1. Первое наблюдаемое состояние $r_1$ началось в момент времени $t=1$.

   2. Последнее наблюдаемое состояние $r_q$ закончилось строго в момент времени $T$.

  Общее предположение (general assumption):

   1. Первое наблюдаемое состояние $r_1$ началось в момент времени $t=1$ или до него.

   2. Последнее наблюдаемое состояние $r_q$ закончилось в момент времени $T$ или после него.

   В случае общего предположения рекурсивная формула для вычисления прямой переменной инициализируется как
      $$\alpha_{\tau}(j,d)=P[S_{[\tau-d+1:\tau]}=j|\lambda]=\pi_{j,d}, \;\;\; \tau \leq 0, d\in D,$$
   где $\pi_{j,d}$ -- предельные вероятности состояний скрытого полумарковского процесса, а в случае упрощенного -- как
   $$\alpha_{0}(j,d)=\pi_{j,d}, \;\;\; d\in D$$
   $$\alpha_{\tau}(j,d)=0\;\;\; \tau<0, d\in D.$$
Заметим, что начальное распределение вероятностей состояний может вводиться как $\pi^\prime_{j,d}=P[S_{[1:d]=j}|\lambda]$, равное $\sum\limits_{i,d^\prime}\pi_{i,d^\prime}a_{(i,d^\prime)(j,d)}$. Тогда выражение для инициализации рекурсии для прямой переменной может быть введена как $\alpha_d(j,d)=\pi^\prime_{j,d}b_{j,d}(o_{1:d})$  для $d\in D$.

%Используя прямую переменную, вероятность генерации

В силу того, что предложенный алгоритм прямого хода оперирует совместными вероятностями, его программная реализация страдает от проблемы антипереполнения или потери значимости (underflow problem). Это происходит из-за того, что совместные вероятности экспоненциально уменьшаются при увеличении длины последовательности.

Традиционным методом решения проблемы антипереполнения является масштабирование прямой переменной путем умножения на достаточно большой коэффициент \cite{Levinson83}, \cite{Cohen97}. Однако, как утверждается в \cite{Murphy} такой способ не гарантирует решение проблемы антипереполнения. ....

Приведем решение задачи оценивания для общей скрытой полумарковской модели при упрощающем предположении, предложенное в \cite{Yu}. Заметим, что хотя в \cite{Yu} Ю выделил несколько различных вариантов предположений о начале и окончании последовательности наблюдаемых состояний, решение задачи оценивания было предложено им в случае упрощающего предположения.
Подход Ю к решению задачи оценивания отличается от традиционного \cite{Rabiner},\cite{Levinson} и использует понятие апостериорных вероятностей.
Именно, в \cite{Yu} вводится в рассмотрение величина $ \overline{\alpha}_t(i,d)$:
 \begin{equation}
\label{eq: chapt1_InverseProbles_Yu_Posterior_Alpha}
\forall 0<t\leq T, \forall (i,d) \in \mathcal{S}\times \mathcal{D}: \;\;\; \overline{\alpha}_t(i,d)\triangleq P[S_{[t-d+1:t]} = i| O_{1:t}],
\end{equation}
в терминах которой записывается рекуррентная формула для вычисления $P[O_{1:t}]$:
 \begin{equation}
\label{eq: chapt1_InverseProbles_Yu_Probability}
P[O_{1:t}]=\sum\limits_{i\in \mathcal{S}}\sum\limits_{d \in \mathcal{D}} P[O_{1:t-d}]\overline{\alpha}_t(i,d)b_{i,d}(O_{t-d+1:t}),
\end{equation}
где, в свою очередь, $ \overline{\alpha}_t(i,d)$ вычисляется тоже рекурсивно:
 \begin{equation}
\label{eq: chapt1_InverseProbles_Yu_Posterior_Alpha_Recursion}
\overline{\alpha}_t(i,d) = \sum\limits_{i^\prime\in S \backslash \{i\}}\sum\limits_{d^\prime \in \mathcal{D}}\overline{\alpha}_{t-d}(i^\prime,d^\prime)\frac{b_{i^\prime,d^\prime}(O_{t-d-d^\prime+1}^{t-d})P[O_{1:t-d-d^\prime}]}{P[O_{1:t-d}]}a_{(i^\prime,d^\prime)(i,d)}.
\end{equation}
Для инициализации рекурсии в случае упрощающего предположения справедливо записать:
 \begin{equation}\label{eq: chapt1_InverseProbles_Yu_Posterior_Alpha_Init0}
\overline{\alpha}_0(j,d) = \pi_{j,d},
\end{equation}
 \begin{equation}\label{eq:chapt1_InverseProbles_Yu_Posterior_Alpha_InitNegative}
\overline{\alpha}_t(j,d) = 0, \;\;\; t<0.
\end{equation}
% Дописать про неапостериорные и про вывод для начальных условий

Таким образом, вероятность наблюдения полной последовательности $P[O_{1:T}]$ можно вычислять по формуле:
\begin{equation}
\label{eq_target_probability}
P[O_{1:T}]=\sum\limits_{i\in \mathcal{S}}\sum\limits_{d \in \mathcal{D}} P[O_{1:T-d}]\overline{\alpha}_T(i,d)b_{i,d}(O_{T-d+1:T}).
\end{equation}
В \cite{Yu} утверждается, что такой способ решения задачи оценивания позволяет избежать потери вычислительной точности в случае ОСПММ и поэтому его можно рекомендовать к применению на практике.
